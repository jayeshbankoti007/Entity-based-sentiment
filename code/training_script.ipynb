{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-16 14:58:17.469524: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2021-10-16 14:58:17.469587: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "from sklearn import metrics, model_selection\n",
    "from model import BERTSentiment\n",
    "import config\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from engine import train_fn, eval_fn\n",
    "import dataset\n",
    "import pandas as pd\n",
    "from engine import calculate_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = pd.read_csv(config.PROCESSED_TRAIN_FILE_PATH).dropna().reset_index(drop=True)\n",
    "test_data = pd.read_csv(config.PROCESSED_TEST_FILE_PATH).dropna().reset_index(drop=True)\n",
    "\n",
    "df_train, df_val = model_selection.train_test_split(\n",
    "    train_data,\n",
    "    test_size = 0.1,\n",
    "    random_state=42,\n",
    "    stratify=train_data.Sentiment.values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "\n",
    "train_dataset = dataset.ExtractDataset(\n",
    "    sentences = (df_train.Sentence.values), \n",
    "    sentiments = (df_train.Sentiment.values), \n",
    "    entity = (df_train.Entity.values)\n",
    ")\n",
    "\n",
    "val_dataset = dataset.ExtractDataset(\n",
    "    sentences = (df_val.Sentence.values), \n",
    "    sentiments = (df_val.Sentiment.values), \n",
    "    entity = (df_val.Entity.values)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=config.TRAIN_BATCH_SIZE, \n",
    "                          shuffle=True)\n",
    "eval_loader = DataLoader(val_dataset, \n",
    "                         batch_size=config.VALID_BATCH_SIZE)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = BERTSentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.bert_layer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-4},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0},\n",
    "]\n",
    "\n",
    "num_training_steps = int(len(df_train)/config.TRAIN_BATCH_SIZE * config.FROZEN_BERT_EPOCHS)\n",
    "\n",
    "optimizer = AdamW(optimizer_parameters, lr=1e-4)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = num_training_steps\n",
    ")\n",
    "\n",
    "best_accuracy = 0\n",
    "early_stopping_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epochs in range(config.FROZEN_BERT_EPOCHS):\n",
    "    print(\"Epoch :\", epochs)\n",
    "    loss, train_accuracy = train_fn(train_loader, model, optimizer, device, scheduler)\n",
    "    print(f\"Total Epoch Train Accuracy : {train_accuracy} with loss : {loss}\")\n",
    "    predicted, labels = eval_fn(eval_loader, model, device)\n",
    "    val_accuracy = calculate_accuracy(predicted, labels, 'epoch')\n",
    "    print(f\"Total Epoch Eval Accuracy : {val_accuracy}\")\n",
    "    if val_accuracy > best_accuracy:\n",
    "        early_stopping_counter = 0\n",
    "        best_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), config.MODEL_SAVE_PATH)\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter > config.FROZEN_BERT_EARLY_STOPPING:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfreeze BERT for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.bert_layer.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-4},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0},\n",
    "]\n",
    "\n",
    "num_training_steps = int(len(df_train)/config.TRAIN_BATCH_SIZE * config.BERT_EPOCHS)\n",
    "\n",
    "optimizer = AdamW(optimizer_parameters, lr=1e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = num_training_steps\n",
    ")\n",
    "\n",
    "early_stopping_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epochs in range(config.BERT_EPOCHS):\n",
    "    print(\"Epoch :\", epochs)\n",
    "    loss, train_accuracy = train_fn(train_loader, model, optimizer, device, scheduler)\n",
    "    print(f\"Total Epoch Train Accuracy : {train_accuracy} with loss : {loss}\")\n",
    "    predicted, labels = eval_fn(eval_loader, model, device)\n",
    "    val_accuracy = calculate_accuracy(predicted, labels, 'epoch')\n",
    "    print(f\"Total Epoch Eval Accuracy : {val_accuracy}\")\n",
    "    if val_accuracy > best_accuracy:\n",
    "        early_stopping_counter = 0\n",
    "        best_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), config.MODEL_SAVE_PATH)\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter > config.BERT_EARLY_STOPPING:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import config\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestDataset:\n",
    "    def __init__(self, sentences, entity):\n",
    "        self.sentences = list(map(self.remove_extra_space, sentences))\n",
    "        self.entity = list(map(self.remove_extra_space, entity))\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "        self.sentence_encodings = self.tokenizer(self.sentences, return_offsets_mapping=True)\n",
    "        self.max_len = config.MAX_LEN\n",
    "\n",
    "    def sentiment_encoder(self, sentiment):\n",
    "        if sentiment == 'positive':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def remove_extra_space(self, text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entity)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        entity = self.entity[idx]\n",
    "\n",
    "        tok_sentence_ids = self.sentence_encodings.input_ids[idx]\n",
    "        tok_sentence_offsets = self.sentence_encodings.offset_mapping[idx][1:-1]\n",
    "        tok_sentence_type_id = self.sentence_encodings.token_type_ids[idx]\n",
    "        tok_sentence_mask = self.sentence_encodings.attention_mask[idx]\n",
    "\n",
    "        start_ids = [i for i in range(len(sentence)) if sentence.startswith(entity, i)]\n",
    "\n",
    "        aspect_word_masking = np.zeros(len(tok_sentence_ids))\n",
    "\n",
    "        word_counter = 0\n",
    "        word_started = 0\n",
    "        for i, (start_id, end_id) in enumerate(tok_sentence_offsets):\n",
    "            if word_started:\n",
    "                aspect_word_masking[i] = 1\n",
    "                if start_ids[word_counter] + len(entity) == end_id:\n",
    "                    word_counter += 1\n",
    "                    word_started = 0\n",
    "            else:\n",
    "                if word_counter < len(start_ids) and start_ids[word_counter] == start_id:\n",
    "                    word_started = 1\n",
    "                    aspect_word_masking[i] = 1\n",
    "                    if start_ids[word_counter] + len(entity) == end_id:\n",
    "                        word_counter += 1\n",
    "                        word_started = 0\n",
    "\n",
    "        # Need to pad them \n",
    "        padding_len = self.max_len - len(tok_sentence_ids)\n",
    "\n",
    "        tok_sentence_ids = tok_sentence_ids + [0] * padding_len\n",
    "        tok_sentence_mask = tok_sentence_mask + [0] * padding_len\n",
    "        tok_sentence_type_id = tok_sentence_type_id + [0] * padding_len\n",
    "        aspect_word_masking = [0] + aspect_word_masking.tolist() + [0] + [0] * (padding_len-2)\n",
    "\n",
    "        tok_sentence_ids = tok_sentence_ids[:self.max_len]\n",
    "        tok_sentence_mask = tok_sentence_mask[:self.max_len]\n",
    "        tok_sentence_type_id = tok_sentence_type_id[:self.max_len]\n",
    "        aspect_word_masking = aspect_word_masking[:self.max_len]\n",
    "\n",
    "        return {\n",
    "            'input_ids' : torch.tensor(tok_sentence_ids, dtype=torch.long),\n",
    "            'attention_mask' : torch.tensor(tok_sentence_mask, dtype=torch.long),\n",
    "            'aspect_word_masking' : torch.tensor(aspect_word_masking, dtype=torch.bool),\n",
    "            'token_type_ids' : torch.tensor(tok_sentence_type_id, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestDataset(\n",
    "    sentences = (test_data.Sentence.values),  \n",
    "    entity = (test_data.Entity.values)\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for batch_index, dataset in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(dataset['input_ids'].to(device), \n",
    "                        dataset['attention_mask'].to(device),\n",
    "                        dataset['token_type_ids'].to(device),\n",
    "                        dataset['aspect_word_masking'].to(device),\n",
    "                    )\n",
    "    predictions.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (np.array(predictions) >= 0.5).astype(int).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['final_sentiment'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv(config.TEST_FILE_SAVE_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f3a9e307020fff06c9d718ee1c49caa3fe3c690079ecb1ba52008f1a0a4e25d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
